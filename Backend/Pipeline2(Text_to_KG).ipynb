{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZN5mQ6s7GYJf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3zBCxdZGurE"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/content/ZenerDiodeBasicsSymbolCharacteristicsApplicationsProsConsExplained')\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCUjbDd7Gvkv"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLI7PoUbG13P"
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziB2bGBrG3oD"
   },
   "outputs": [],
   "source": [
    "!pip install nltk spacy transformers\n",
    "!python -m nltk.downloader punkt stopwords\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R7hjRhIG50F"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "rebel_model_name = \"Babelscape/rebel-large\"\n",
    "rebel_tokenizer = AutoTokenizer.from_pretrained(rebel_model_name)\n",
    "rebel_model = AutoModelForSeq2SeqLM.from_pretrained(rebel_model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'[\"“”‘’]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def extract_rebel_triplets(decoded_text):\n",
    "    triples = []\n",
    "    try:\n",
    "        segments = decoded_text.split('<triplet>')\n",
    "        for segment in segments:\n",
    "            if segment.strip() == \"\":\n",
    "                continue\n",
    "            parts = segment.strip().split('<subj>')\n",
    "            if len(parts) < 2: continue\n",
    "            head = parts[0].strip()\n",
    "\n",
    "            rest = parts[1].split('<obj>')\n",
    "            if len(rest) < 2: continue\n",
    "            relation = rest[0].strip()\n",
    "            tail = rest[1].strip()\n",
    "\n",
    "            head = re.sub(r'</?[a-z]+>', '', head).strip()\n",
    "            relation = re.sub(r'</?[a-z]+>', '', relation).strip()\n",
    "            tail = re.sub(r'</?[a-z]+>', '', tail).strip()\n",
    "\n",
    "            if head and relation and tail:\n",
    "                triples.append((head, relation, tail))\n",
    "    except Exception as e:\n",
    "        print(f\"REBEL parsing error: {e}\")\n",
    "    return triples\n",
    "def get_entity_type(entity_text, ner_results):\n",
    "    for ent in ner_results:\n",
    "        if entity_text.lower() in ent['word'].lower():\n",
    "            return ent['entity_group']\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def process_dataframe(df):\n",
    "    all_enriched_triples = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "        original_text = row['text']\n",
    "        clean_text = preprocess_text(original_text)\n",
    "\n",
    "        inputs = rebel_tokenizer(clean_text, return_tensors=\"pt\", truncation=True, max_length=512).to(rebel_model.device)\n",
    "        outputs = rebel_model.generate(**inputs, max_new_tokens=512, num_beams=5)\n",
    "        decoded_output = rebel_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "        triples = extract_rebel_triplets(decoded_output)\n",
    "\n",
    "        ner_results = ner_pipeline(original_text)\n",
    "\n",
    "        enriched_triples = []\n",
    "        for head, relation, tail in triples:\n",
    "            head_type = get_entity_type(head, ner_results)\n",
    "            tail_type = get_entity_type(tail, ner_results)\n",
    "\n",
    "            enriched_triples.append({\n",
    "                'video_id': row['video_id'],\n",
    "                'start_time': row['start_time'],\n",
    "                'end_time': row['end_time'],\n",
    "                'head': head,\n",
    "                'head_type': head_type,\n",
    "                'relation': relation,\n",
    "                'tail': tail,\n",
    "                'tail_type': tail_type,\n",
    "                'text': original_text\n",
    "            })\n",
    "\n",
    "        all_enriched_triples.append(enriched_triples)\n",
    "\n",
    "    df['enriched_triples'] = all_enriched_triples\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvKS2BGsG6pj"
   },
   "outputs": [],
   "source": [
    "final_df = process_dataframe(df)\n",
    "from pprint import pprint\n",
    "pprint(final_df['enriched_triples'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmTnb1uoG9I0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_noisy_triples(triples):\n",
    "    fixed_triples = []\n",
    "\n",
    "    for triple in triples:\n",
    "        head = triple['head'].strip()\n",
    "        relation = triple['relation'].strip()\n",
    "        tail = triple['tail'].strip()\n",
    "        head_type = triple.get('head_type', 'UNKNOWN')\n",
    "        tail_type = triple.get('tail_type', 'UNKNOWN')\n",
    "\n",
    "        head = head.replace('</s>', '').strip()\n",
    "        relation = relation.replace('</s>', '').strip()\n",
    "        tail = tail.replace('</s>', '').strip()\n",
    "        if not head or not tail or not relation:\n",
    "            continue\n",
    "        if head.lower() == tail.lower():\n",
    "            continue\n",
    "        if relation.lower() in [head.lower(), tail.lower()]:\n",
    "            continue\n",
    "        tail_as_relation_patterns = ['part of', 'type of', 'used for', 'used by', 'based on', 'known as']\n",
    "        if any(tail.lower().startswith(p) for p in tail_as_relation_patterns):\n",
    "            relation, tail = tail, relation\n",
    "        if head_type == 'UNKNOWN' and tail_type == 'UNKNOWN' and len(relation.split()) > 4:\n",
    "            continue\n",
    "        relation = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', relation, flags=re.IGNORECASE)\n",
    "\n",
    "        fixed_triple = triple.copy()\n",
    "        fixed_triple.update({\n",
    "            'head': head,\n",
    "            'relation': relation,\n",
    "            'tail': tail\n",
    "        })\n",
    "        fixed_triples.append(fixed_triple)\n",
    "\n",
    "    return fixed_triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28mtUYCxG_uk"
   },
   "outputs": [],
   "source": [
    "df['enriched_triples'] = df['enriched_triples'].apply(fix_noisy_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcLedVq6HDHr"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def validate_triple_pos(triple):\n",
    "    \"\"\"\n",
    "    POS tag check for REBEL triples.\n",
    "    - Head and Tail should be NOUN/PROPN\n",
    "    - Relation should be VERB/ADP/ADJ\n",
    "    \"\"\"\n",
    "    head_doc = nlp(triple['head'])\n",
    "    tail_doc = nlp(triple['tail'])\n",
    "    rel_doc = nlp(triple['relation'])\n",
    "\n",
    "    head_valid = any(token.pos_ in ['NOUN', 'PROPN'] for token in head_doc)\n",
    "    tail_valid = any(token.pos_ in ['NOUN', 'PROPN'] for token in tail_doc)\n",
    "    rel_valid = any(token.pos_ in ['VERB', 'ADP', 'ADJ'] for token in rel_doc)\n",
    "\n",
    "    return head_valid and tail_valid and rel_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjjlGdPeHH80"
   },
   "outputs": [],
   "source": [
    "df['validated_triples'] = df['enriched_triples'].apply(\n",
    "    lambda triples: [triple for triple in triples if validate_triple_pos(triple)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmFwN0yyHJyL"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQnV-B_RHLp4"
   },
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHA4g9sLHOI2"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\"  )  # or keep your preferred 384D/128D model\n",
    "\n",
    "def embed_segments(df):\n",
    "    df[\"segment_embedding\"] = df[\"text\"].apply(lambda x: model.encode(x, normalize_embeddings=True).tolist())\n",
    "    return df\n",
    "\n",
    "final_df = embed_segments(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UARGMm07HQY-"
   },
   "outputs": [],
   "source": [
    "final_df[\"enriched_triples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOMo-mnVHSH1"
   },
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHZxcC3QHVJ7"
   },
   "outputs": [],
   "source": [
    "!pip install Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gefz9k_YHWyZ"
   },
   "outputs": [],
   "source": [
    "URI=\"Your URI\"\n",
    "NEO4J_USERNAME=\"USERNAME\"\n",
    "NEO4J_PASSWORD=\"Password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-Ee7kTyHZQs"
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import ast\n",
    "\n",
    "driver = GraphDatabase.driver(URI,auth=(\"neo4j\",NEO4J_PASSWORD))\n",
    "\n",
    "def push_to_neo4j(tx, video_id, start_time, end_time, text, embedding, triples):\n",
    "    #section_id = f\"{video_id}_{start_time:.2f}_{end_time:.2f}\"\n",
    "    section_id = f\"{video_id}_{float(start_time):.2f}_{float(end_time):.2f}\"\n",
    "\n",
    "    # MERGE the Video node first (avoids duplicates)\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (v:Video {video_id: $video_id})\n",
    "        ON CREATE SET v.name = $video_id\n",
    "    \"\"\", video_id=video_id)\n",
    "\n",
    "    # Create Section node and set attributes\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (s:Section {id: $section_id})\n",
    "        SET s.video_id = $video_id,\n",
    "            s.start_time = $start_time,\n",
    "            s.end_time = $end_time,\n",
    "            s.text = $text,\n",
    "            s.embedding = $embedding\n",
    "    \"\"\", section_id=section_id, video_id=video_id,\n",
    "         start_time=start_time, end_time=end_time,\n",
    "         text=text, embedding=embedding)\n",
    "\n",
    "    # Link Section to the Video\n",
    "    tx.run(\"\"\"\n",
    "        MATCH (v:Video {video_id: $video_id})\n",
    "        MATCH (s:Section {id: $section_id})\n",
    "        MERGE (v)-[:HAS_SECTION]->(s)\n",
    "    \"\"\", video_id=video_id, section_id=section_id)\n",
    "\n",
    "    for triple in triples:\n",
    "        head = triple['head']\n",
    "        tail = triple['tail']\n",
    "        relation = triple['relation']\n",
    "        head_type = triple.get('head_type', 'UNKNOWN')\n",
    "        tail_type = triple.get('tail_type', 'UNKNOWN')\n",
    "\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (h:Entity {name: $head})\n",
    "            ON CREATE SET h.type = $head_type\n",
    "        \"\"\", head=head, head_type=head_type)\n",
    "\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (t:Entity {name: $tail})\n",
    "            ON CREATE SET t.type = $tail_type\n",
    "        \"\"\", tail=tail, tail_type=tail_type)\n",
    "\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (h:Entity {name: $head})\n",
    "            MATCH (t:Entity {name: $tail})\n",
    "            MERGE (h)-[r:RELATION {type: $relation}]->(t)\n",
    "        \"\"\", head=head, tail=tail, relation=relation)\n",
    "\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (s:Section {id: $section_id})\n",
    "            MATCH (h:Entity {name: $head})\n",
    "            MATCH (t:Entity {name: $tail})\n",
    "            MERGE (s)-[:MENTIONS]->(h)\n",
    "            MERGE (s)-[:MENTIONS]->(t)\n",
    "            MERGE (s)-[:MENTIONS_RELATION {type: $relation}]->(h)\n",
    "            MERGE (s)-[:MENTIONS_RELATION {type: $relation}]->(t)\n",
    "        \"\"\", section_id=section_id, head=head, tail=tail, relation=relation)\n",
    "\n",
    "with driver.session() as session:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        video_id = row['video_id']\n",
    "        start_time = row['start_time']\n",
    "        end_time = row['end_time']\n",
    "        text = row['text']\n",
    "        embedding = row['segment_embedding']\n",
    "\n",
    "        triples = row['enriched_triples']\n",
    "        if isinstance(triples, str):\n",
    "            triples = ast.literal_eval(triples)\n",
    "\n",
    "        session.write_transaction(push_to_neo4j, video_id, start_time, end_time, text, embedding, triples)\n",
    "\n",
    "driver.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
